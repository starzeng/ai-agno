import asyncio
import json
from textwrap import dedent
from typing import Dict, Optional

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow.workflow import Workflow
from pydantic import BaseModel, Field


# --- Response Models ---
class NewsArticle(BaseModel):
    title: str = Field(..., description="æ–‡ç« æ ‡é¢˜ã€‚")
    url: str = Field(..., description="æ–‡ç« é“¾æ¥ã€‚")
    summary: Optional[str] = Field(
        ..., description="æ–‡ç« æ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰ã€‚"
    )


class SearchResults(BaseModel):
    articles: list[NewsArticle]


class ScrapedArticle(BaseModel):
    title: str = Field(..., description="æ–‡ç« æ ‡é¢˜ã€‚")
    url: str = Field(..., description="æ–‡ç« é“¾æ¥ã€‚")
    summary: Optional[str] = Field(
        ..., description="æ–‡ç« æ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰ã€‚"
    )
    content: Optional[str] = Field(
        ...,
        description="æ–‡ç« å®Œæ•´å†…å®¹ï¼ˆMarkdown æ ¼å¼ï¼‰ã€‚å¦‚æœå†…å®¹ä¸å¯ç”¨åˆ™ä¸º Noneã€‚",
    )


# --- Agents ---
research_agent = Agent(
    name="åšå®¢è°ƒç ”Agent",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[GoogleSearchTools()],
    description=dedent("""\
    ä½ æ˜¯ BlogResearch-Xï¼Œä¸€åä¸“ä¸šçš„è°ƒç ”åŠ©ç†ï¼Œæ“…é•¿ä¸ºä¼˜è´¨åšå®¢å†…å®¹å‘ç°æ¥æºã€‚ä½ çš„ä¸“é•¿åŒ…æ‹¬ï¼š

    - æ‰¾åˆ°æƒå¨å’Œè¶‹åŠ¿æ¥æº
    - è¯„ä¼°å†…å®¹çš„å¯ä¿¡æ€§å’Œç›¸å…³æ€§
    - è¯†åˆ«å¤šæ ·è§‚ç‚¹å’Œä¸“å®¶æ„è§
    - å‘ç°ç‹¬ç‰¹çš„åˆ‡å…¥ç‚¹å’Œæ´è§
    - ç¡®ä¿ä¸»é¢˜è¦†ç›–å…¨é¢
    """),
    instructions=dedent("""\
    1. æœç´¢ç­–ç•¥ ğŸ”
       - æŸ¥æ‰¾ 10-15 ä¸ªç›¸å…³æ¥æºå¹¶ç­›é€‰å‡º 5-7 ä¸ªæœ€ä½³æ¥æº
       - ä¼˜å…ˆæœ€è¿‘ä¸”æƒå¨çš„å†…å®¹
       - å¯»æ‰¾ç‹¬ç‰¹çš„è§’åº¦å’Œä¸“å®¶æ´è§
    2. æ¥æºè¯„ä¼° ğŸ“Š
       - éªŒè¯æ¥æºçš„å¯ä¿¡åº¦ä¸ä¸“ä¸šæ€§
       - æ£€æŸ¥å‘å¸ƒæ—¶é—´ä»¥ä¿è¯æ—¶æ•ˆæ€§
       - è¯„ä¼°å†…å®¹æ·±åº¦ä¸ç‹¬ç‰¹æ€§
    3. è§‚ç‚¹å¤šæ ·æ€§ ğŸŒ
       - åŒ…æ‹¬ä¸åŒçš„è§‚ç‚¹
       - æ±‡é›†ä¸»æµä¸ä¸“å®¶æ„è§
       - å¯»æ‰¾æ”¯æŒæ€§æ•°æ®ä¸ç»Ÿè®¡
    """),
    output_schema=SearchResults,
)

content_scraper_agent = Agent(
    name="å†…å®¹æŠ“å–Agent",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[Newspaper4kTools()],
    description=dedent("""\
    ä½ æ˜¯ ContentBot-Xï¼Œä¸“æ³¨äºæå–ä¸å¤„ç†æ•°å­—å†…å®¹ä»¥ä¾›åšå®¢å†™ä½œã€‚ä½ çš„ä¸“é•¿åŒ…æ‹¬ï¼š

    - é«˜æ•ˆçš„å†…å®¹æå–
    - æ™ºèƒ½æ ¼å¼åŒ–ä¸ç»“æ„åŒ–
    - è¯†åˆ«å…³é”®ä¿¡æ¯
    - ä¿ç•™å¼•ç”¨ä¸ç»Ÿè®¡æ•°æ®
    - ä¿æŒæ¥æºå½’å±
    """),
    instructions=dedent("""\
    1. å†…å®¹æå– ğŸ“‘
       - ä»æ–‡ç« ä¸­æå–å†…å®¹
       - ä¿ç•™é‡è¦å¼•ç”¨ä¸ç»Ÿè®¡æ•°æ®
       - ä¿æŒæ­£ç¡®çš„å½’å±ä¿¡æ¯
       - ä¼˜é›…å¤„ç†ä»˜è´¹å¢™
    2. å†…å®¹å¤„ç† ğŸ”„
       - å°†æ–‡æœ¬æ ¼å¼åŒ–ä¸ºå¹²å‡€çš„ Markdown
       - ä¿ç•™å…³é”®ä¿¡æ¯
       - åˆç†ç»„ç»‡ç»“æ„
    3. è´¨é‡æ§åˆ¶ âœ…
       - éªŒè¯å†…å®¹çš„ç›¸å…³æ€§
       - ç¡®ä¿æå–å‡†ç¡®
       - ä¿æŒå¯è¯»æ€§
    """),
    output_schema=ScrapedArticle,
)

blog_writer_agent = Agent(
    name="åšå®¢æ’°å†™Agent",
    model=OpenAIChat(id="gpt-5-mini"),
    description=dedent("""\
    ä½ æ˜¯ BlogMaster-Xï¼Œä¸€åå°†æ–°é—»å†™ä½œä¸æ•°å­—è¥é”€ç»“åˆçš„é«˜çº§å†…å®¹åˆ›ä½œè€…ã€‚ä½ çš„ä¼˜åŠ¿åŒ…æ‹¬ï¼š

    - æ‰“é€ ç—…æ¯’çº§æ ‡é¢˜
    - å†™å‡ºå¸å¼•äººçš„å¼€å¤´
    - ä¸ºæ•°å­—é˜…è¯»ç»“æ„åŒ–å†…å®¹
    - æ— ç¼æ•´åˆè°ƒç ”å†…å®¹
    - åœ¨ä¿è¯è´¨é‡çš„åŒæ—¶ä¼˜åŒ– SEO
    - æä¾›å¯åˆ†äº«çš„ç»“è®º
    """),
    instructions=dedent("""\
    1. å†…å®¹ç­–ç•¥ ğŸ“
       - æ‰“é€ å¸å¼•æ³¨æ„åŠ›çš„æ ‡é¢˜
       - å†™å‡ºå¼•äººå…¥èƒœçš„å¼•è¨€
       - ç»“æ„åŒ–ä»¥æå‡å‚ä¸åº¦
       - åŒ…å«ç›¸å…³å°æ ‡é¢˜
    2. å†™ä½œç²¾ç‚¼ âœï¸
       - åœ¨ä¸“ä¸šæ€§ä¸æ˜“è¯»æ€§ä¹‹é—´å–å¾—å¹³è¡¡
       - ä½¿ç”¨æ¸…æ™°ã€å¸å¼•äººçš„è¯­è¨€
       - æä¾›ç›¸å…³ç¤ºä¾‹
       - è‡ªç„¶åœ°èå…¥ç»Ÿè®¡æ•°æ®
    3. æ¥æºæ•´åˆ ğŸ”
       - æ­£ç¡®å¼•ç”¨æ¥æº
       - åŒ…å«ä¸“å®¶å¼•ç”¨
       - ä¿æŒäº‹å®å‡†ç¡®
    4. æ•°å­—ä¼˜åŒ– ğŸ’»
       - ä¾¿äºæµè§ˆçš„ç»“æ„
       - åŒ…å«å¯åˆ†äº«çš„è¦ç‚¹
       - ä¼˜åŒ– SEO
       - æ·»åŠ å¸å¼•äººçš„å°æ ‡é¢˜

    è¯·æŒ‰ä»¥ä¸‹ç»“æ„è¾“å‡ºåšå®¢æ–‡ç« ï¼š
    # {ç—…æ¯’çº§æ ‡é¢˜}

    ## å¼•è¨€
    {å¼•äººå…¥èƒœçš„é’©å­å’ŒèƒŒæ™¯}

    ## {å¼•äººæ³¨ç›®çš„éƒ¨åˆ† 1}
    {å…³é”®è§è§£ä¸åˆ†æ}
    {ä¸“å®¶å¼•ç”¨ä¸ç»Ÿè®¡æ•°æ®}

    ## {å¼•äººå…¥èƒœçš„éƒ¨åˆ† 2}
    {æ›´æ·±å…¥çš„æ¢è®¨}
    {ç°å®ä¸–ç•Œæ¡ˆä¾‹}

    ## {å®ç”¨éƒ¨åˆ† 3}
    {å¯æ“ä½œçš„è§è§£}
    {ä¸“å®¶å»ºè®®}

    ## å…³é”®è¦ç‚¹
    - {å¯åˆ†äº«çš„è§è§£ 1}
    - {å®ç”¨è¦ç‚¹ 2}
    - {é‡è¦å‘ç° 3}

    ## æ¥æº
    {å¸¦é“¾æ¥çš„æ­£ç¡®å½’å±æ¥æº}
    """),
    markdown=True,
)


# --- Helper Functions ---
def get_cached_blog_post(session_state, topic: str) -> Optional[str]:
    """ä» workflow ä¼šè¯çŠ¶æ€ä¸­è·å–å·²ç¼“å­˜çš„åšå®¢æ–‡ç« """
    logger.info("æ£€æŸ¥æ˜¯å¦å­˜åœ¨å·²ç¼“å­˜çš„åšå®¢æ–‡ç« ")
    return session_state.get("blog_posts", {}).get(topic)


def cache_blog_post(session_state, topic: str, blog_post: str):
    """å°†åšå®¢æ–‡ç« ç¼“å­˜åˆ° workflow ä¼šè¯çŠ¶æ€"""
    logger.info(f"ä¿å­˜åšå®¢æ–‡ç« ï¼Œä¸»é¢˜: {topic}")
    if "blog_posts" not in session_state:
        session_state["blog_posts"] = {}
    session_state["blog_posts"][topic] = blog_post


def get_cached_search_results(session_state, topic: str) -> Optional[SearchResults]:
    """ä» workflow ä¼šè¯çŠ¶æ€ä¸­è·å–å·²ç¼“å­˜çš„æœç´¢ç»“æœ"""
    logger.info("æ£€æŸ¥æ˜¯å¦å­˜åœ¨å·²ç¼“å­˜çš„æœç´¢ç»“æœ")
    search_results = session_state.get("search_results", {}).get(topic)
    if search_results and isinstance(search_results, dict):
        try:
            return SearchResults.model_validate(search_results)
        except Exception as e:
            logger.warning(f"æ— æ³•éªŒè¯å·²ç¼“å­˜çš„æœç´¢ç»“æœ: {e}")
    return search_results if isinstance(search_results, SearchResults) else None


def cache_search_results(session_state, topic: str, search_results: SearchResults):
    """å°†æœç´¢ç»“æœç¼“å­˜åˆ° workflow ä¼šè¯çŠ¶æ€"""
    logger.info(f"ä¿å­˜æœç´¢ç»“æœï¼Œä¸»é¢˜: {topic}")
    if "search_results" not in session_state:
        session_state["search_results"] = {}
    session_state["search_results"][topic] = search_results.model_dump()


def get_cached_scraped_articles(
        session_state, topic: str
) -> Optional[Dict[str, ScrapedArticle]]:
    """ä» workflow ä¼šè¯çŠ¶æ€ä¸­è·å–å·²ç¼“å­˜çš„æŠ“å–æ–‡ç« """
    logger.info("æ£€æŸ¥æ˜¯å¦å­˜åœ¨å·²ç¼“å­˜çš„æŠ“å–æ–‡ç« ")
    scraped_articles = session_state.get("scraped_articles", {}).get(topic)
    if scraped_articles and isinstance(scraped_articles, dict):
        try:
            return {
                url: ScrapedArticle.model_validate(article)
                for url, article in scraped_articles.items()
            }
        except Exception as e:
            logger.warning(f"æ— æ³•éªŒè¯å·²ç¼“å­˜çš„æŠ“å–æ–‡ç« : {e}")
    return scraped_articles if isinstance(scraped_articles, dict) else None


def cache_scraped_articles(
        session_state, topic: str, scraped_articles: Dict[str, ScrapedArticle]
):
    """å°†æŠ“å–çš„æ–‡ç« ç¼“å­˜åˆ° workflow ä¼šè¯çŠ¶æ€"""
    logger.info(f"ä¿å­˜æŠ“å–æ–‡ç« ï¼Œä¸»é¢˜: {topic}")
    if "scraped_articles" not in session_state:
        session_state["scraped_articles"] = {}
    session_state["scraped_articles"][topic] = {
        url: article.model_dump() for url, article in scraped_articles.items()
    }


async def get_search_results(
        session_state, topic: str, use_cache: bool = True, num_attempts: int = 3
) -> Optional[SearchResults]:
    """å¸¦ç¼“å­˜æ”¯æŒçš„æœç´¢ç»“æœè·å–å‡½æ•°"""

    # ä¼˜å…ˆæ£€æŸ¥ç¼“å­˜
    if use_cache:
        cached_results = get_cached_search_results(session_state, topic)
        if cached_results:
            logger.info(f"åœ¨ç¼“å­˜ä¸­æ‰¾åˆ° {len(cached_results.articles)} ç¯‡æ–‡ç« ã€‚")
            return cached_results

    # æœç´¢æ–°ç»“æœ
    for attempt in range(num_attempts):
        try:
            print(
                f"ğŸ” æ­£åœ¨æœç´¢æœ‰å…³ä¸»é¢˜çš„æ–‡ç« : {topic}ï¼ˆå°è¯• {attempt + 1}/{num_attempts}ï¼‰"
            )
            response = await research_agent.arun(topic)

            if (
                    response
                    and response.content
                    and isinstance(response.content, SearchResults)
            ):
                article_count = len(response.content.articles)
                logger.info(f"åœ¨ç¬¬ {attempt + 1} æ¬¡å°è¯•ä¸­æ‰¾åˆ° {article_count} ç¯‡æ–‡ç« ")
                print(f"âœ… æ‰¾åˆ° {article_count} ç¯‡ç›¸å…³æ–‡ç« ")

                # ç¼“å­˜ç»“æœ
                cache_search_results(session_state, topic, response.content)
                return response.content
            else:
                logger.warning(
                    f"å°è¯• {attempt + 1}/{num_attempts} å¤±è´¥: è¿”å›ç±»å‹æ— æ•ˆ"
                )

        except Exception as e:
            logger.warning(f"å°è¯• {attempt + 1}/{num_attempts} å¤±è´¥: {str(e)}")

    logger.error(f"åœ¨ {num_attempts} æ¬¡å°è¯•åæœªèƒ½è·å–æœç´¢ç»“æœ")
    return None


async def scrape_articles(
        session_state,
        topic: str,
        search_results: SearchResults,
        use_cache: bool = True,
) -> Dict[str, ScrapedArticle]:
    """å¸¦ç¼“å­˜æ”¯æŒçš„æ–‡ç« æŠ“å–å‡½æ•°"""

    # ä¼˜å…ˆæ£€æŸ¥ç¼“å­˜
    if use_cache:
        cached_articles = get_cached_scraped_articles(session_state, topic)
        if cached_articles:
            logger.info(f"åœ¨ç¼“å­˜ä¸­æ‰¾åˆ° {len(cached_articles)} ç¯‡æŠ“å–æ–‡ç« ã€‚")
            return cached_articles

    scraped_articles: Dict[str, ScrapedArticle] = {}

    print(f"ğŸ“„ æ­£åœ¨æŠ“å– {len(search_results.articles)} ç¯‡æ–‡ç« ...")

    for i, article in enumerate(search_results.articles, 1):
        try:
            print(
                f"ğŸ“– æ­£åœ¨æŠ“å–ç¬¬ {i}/{len(search_results.articles)} ç¯‡æ–‡ç« : {article.title[:50]}..."
            )
            response = await content_scraper_agent.arun(article.url)

            if (
                    response
                    and response.content
                    and isinstance(response.content, ScrapedArticle)
            ):
                scraped_articles[response.content.url] = response.content
                logger.info(f"å·²æŠ“å–æ–‡ç« : {response.content.url}")
                print(f"âœ… æˆåŠŸæŠ“å–: {response.content.title[:50]}...")
            else:
                print(f"âŒ æŠ“å–å¤±è´¥: {article.title[:50]}...")

        except Exception as e:
            logger.warning(f"æŠ“å– {article.url} å¤±è´¥: {str(e)}")
            print(f"âŒ æŠ“å–å‡ºé”™: {article.title[:50]}...")

    # ç¼“å­˜æŠ“å–åˆ°çš„æ–‡ç« 
    cache_scraped_articles(session_state, topic, scraped_articles)
    return scraped_articles


# --- Main Execution Function ---
async def blog_generation_execution(
        session_state,
        topic: str = None,
        use_search_cache: bool = True,
        use_scrape_cache: bool = True,
        use_blog_cache: bool = True,
) -> str:
    """
    åšå®¢æ–‡ç« ç”Ÿæˆå·¥ä½œæµæ‰§è¡Œå‡½æ•°ã€‚

    å‚æ•°:
        session_state: å…±äº«ä¼šè¯çŠ¶æ€
        topic: åšå®¢ä¸»é¢˜ï¼ˆè‹¥æœªæä¾›ï¼Œåˆ™ä½¿ç”¨ execution_input.inputï¼‰
        use_search_cache: æ˜¯å¦ä½¿ç”¨å·²ç¼“å­˜çš„æœç´¢ç»“æœ
        use_scrape_cache: æ˜¯å¦ä½¿ç”¨å·²ç¼“å­˜çš„æŠ“å–æ–‡ç« 
        use_blog_cache: æ˜¯å¦ä½¿ç”¨å·²ç¼“å­˜çš„åšå®¢æ–‡ç« 
    """

    blog_topic = topic

    if not blog_topic:
        return "âŒ æœªæä¾›åšå®¢ä¸»é¢˜ã€‚è¯·æŒ‡å®šä¸€ä¸ªä¸»é¢˜ã€‚"

    print(f"ğŸ¨ æ­£åœ¨ä¸ºä¸»é¢˜ç”Ÿæˆåšå®¢æ–‡ç« : {blog_topic}")
    print("=" * 60)

    # ä¼˜å…ˆæ£€æŸ¥å·²ç¼“å­˜çš„åšå®¢æ–‡ç« 
    if use_blog_cache:
        cached_blog = get_cached_blog_post(session_state, blog_topic)
        if cached_blog:
            print("ğŸ“‹ æ‰¾åˆ°å·²ç¼“å­˜çš„åšå®¢æ–‡ç« ï¼")
            return cached_blog

    # é˜¶æ®µ 1: è°ƒç ”å¹¶æ”¶é›†æ¥æº
    print("\nğŸ” ç¬¬ 1 é˜¶æ®µï¼šè°ƒç ”ä¸æ¥æºæ”¶é›†")
    print("=" * 50)

    search_results = await get_search_results(
        session_state, blog_topic, use_search_cache
    )

    if not search_results or len(search_results.articles) == 0:
        return f"âŒ æŠ±æ­‰ï¼Œæœªèƒ½æ‰¾åˆ°ä¸ä¸»é¢˜ç›¸å…³çš„æ–‡ç« : {blog_topic}"

    print(f"ğŸ“Š æ‰¾åˆ°äº† {len(search_results.articles)} ä¸ªç›¸å…³æ¥æºï¼š")
    for i, article in enumerate(search_results.articles, 1):
        print(f"   {i}. {article.title[:60]}...")

    # é˜¶æ®µ 2: å†…å®¹æå–
    print("\nğŸ“„ ç¬¬ 2 é˜¶æ®µï¼šå†…å®¹æå–")
    print("=" * 50)

    scraped_articles = await scrape_articles(
        session_state, blog_topic, search_results, use_scrape_cache
    )

    if not scraped_articles:
        return f"âŒ æœªèƒ½ä»ä»»ä½•æ–‡ç« ä¸­æå–åˆ°å†…å®¹ï¼Œä¸»é¢˜: {blog_topic}"

    print(f"ğŸ“– æˆåŠŸä» {len(scraped_articles)} ç¯‡æ–‡ç« æå–å†…å®¹")

    # é˜¶æ®µ 3: åšå®¢æ’°å†™
    print("\nâœï¸ ç¬¬ 3 é˜¶æ®µï¼šåšå®¢æ’°å†™")
    print("=" * 50)

    # ä¸ºæ’°å†™å™¨å‡†å¤‡è¾“å…¥
    writer_input = {
        "topic": blog_topic,
        "articles": [article.model_dump() for article in scraped_articles.values()],
    }

    print("ğŸ¤– AI æ­£åœ¨ä¸ºä½ æ’°å†™åšå®¢æ–‡ç« ...")
    writer_response = await blog_writer_agent.arun(json.dumps(writer_input, indent=2))

    if not writer_response or not writer_response.content:
        return f"âŒ æ— æ³•ä¸ºä¸»é¢˜ç”Ÿæˆåšå®¢æ–‡ç« : {blog_topic}"

    blog_post = writer_response.content

    # ç¼“å­˜åšå®¢æ–‡ç« 
    cache_blog_post(session_state, blog_topic, blog_post)

    print("âœ… åšå®¢æ–‡ç« ç”ŸæˆæˆåŠŸï¼")
    print(f"ğŸ“ é•¿åº¦: {len(blog_post)} å­—ç¬¦")
    print(f"ğŸ“š æ¥æºæ•°é‡: {len(scraped_articles)} ç¯‡æ–‡ç« ")

    return blog_post


# --- Workflow Definition ---
blog_generator_workflow = Workflow(
    name="åšå®¢æ–‡ç« ç”Ÿæˆå™¨",
    description="å…·æœ‰è°ƒç ”ä¸å†…å®¹åˆ›ä½œèƒ½åŠ›çš„é«˜çº§åšå®¢ç”Ÿæˆå™¨",
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/blog_generator.db",
    ),
    steps=blog_generation_execution,
    session_state={},  # åˆå§‹åŒ–ç©ºçš„ä¼šè¯çŠ¶æ€ä»¥ç”¨äºç¼“å­˜
)

if __name__ == "__main__":
    import random


    async def main():
        # ç”¨æ¥å±•ç¤ºç”Ÿæˆå™¨å¤šæ ·æ€§çš„ç¤ºä¾‹ä¸»é¢˜
        example_topics = [
            "é€šç”¨äººå·¥æ™ºèƒ½çš„å´›èµ·ï¼šæœ€æ–°çªç ´",
            "é‡å­è®¡ç®—å¦‚ä½•é©æ–°ç½‘ç»œå®‰å…¨",
            "2024 å¹´çš„å¯æŒç»­ç”Ÿæ´»ï¼šé™ä½ç¢³è¶³è¿¹çš„å®ç”¨æŠ€å·§",
            "å·¥ä½œçš„æœªæ¥ï¼šäººå·¥æ™ºèƒ½ä¸äººç±»åä½œ",
            "å¤ªç©ºæ—…æ¸¸ï¼šä»ç§‘å¹»åˆ°ç°å®",
            "æ•°å­—æ—¶ä»£çš„æ­£å¿µä¸å¿ƒç†å¥åº·",
            "ç”µåŠ¨æ±½è½¦çš„æ¼”è¿›ï¼šç°çŠ¶ä¸æœªæ¥è¶‹åŠ¿",
            "ä¸ºä»€ä¹ˆçŒ«æ‚„ç„¶ç»Ÿæ²»äº†äº’è”ç½‘",
            "ä¸ºä»€ä¹ˆæ¯”è¨åœ¨å‡Œæ™¨ä¸¤ç‚¹åƒèµ·æ¥æ›´ç¾å‘³çš„ç§‘å­¦",
            "æ©¡èƒ¶é¸­å¦‚ä½•é©æ–°è½¯ä»¶å¼€å‘",
        ]

        # éšæœºæµ‹è¯•ä¸€ä¸ªä¸»é¢˜
        topic = random.choice(example_topics)

        print("ğŸ§ª æµ‹è¯• åšå®¢ç”Ÿæˆå™¨ v2.0")
        print("=" * 60)
        print(f"ğŸ“ ä¸»é¢˜: {topic}")
        print()

        # ç”Ÿæˆåšå®¢æ–‡ç« 
        resp = await blog_generator_workflow.arun(
            topic=topic,
            use_search_cache=True,
            use_scrape_cache=True,
            use_blog_cache=True,
        )

        pprint_run_response(resp, markdown=True, show_time=True)


    asyncio.run(main())

# TODO æœªæµ‹è¯•
